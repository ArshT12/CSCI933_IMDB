{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMRHI+diE8a2ofW6Wc0aZ3l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KHEiQYmhKkD7","executionInfo":{"status":"ok","timestamp":1716967047310,"user_tz":-600,"elapsed":32560,"user":{"displayName":"Arsh.T","userId":"00431916674490476738"}},"outputId":"b1f42398-ddff-447e-aeba-1a022ae87964"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Attention, Concatenate, Dropout\n","from keras.models import Model\n","from keras.utils import to_categorical\n","from keras.preprocessing.text import text_to_word_sequence\n","\n"],"metadata":{"id":"Rx-djo5cKwQP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_dir = '/content/drive/My Drive/ML Assinment 2/assinment 2 part 2/Project/aclImdb/'\n","\n","# Load data function\n","def load_data_from_dir(data_dir, sentiment):\n","    sentences = []\n","    sentiments = []\n","    for file_name in os.listdir(data_dir):\n","        if file_name.endswith(\".txt\"):\n","            with open(os.path.join(data_dir, file_name), 'r', encoding='utf-8') as file:\n","                sentences.append(file.read())\n","                sentiments.append(sentiment)\n","    return sentences, sentiments\n","\n","# Load train and test data\n","rain_pos_dir = os.path.join(data_dir, 'train/pos')\n","train_neg_dir = os.path.join(data_dir, 'train/neg')\n","test_pos_dir = os.path.join(data_dir, 'test/pos')\n","test_neg_dir = os.path.join(data_dir, 'test/neg')\n","\n","# Load data\n","train_pos_sentences, train_pos_sentiments = load_data_from_dir(train_pos_dir, 1)\n","train_neg_sentences, train_neg_sentiments = load_data_from_dir(train_neg_dir, 0)\n","test_pos_sentences, test_pos_sentiments = load_data_from_dir(test_pos_dir, 1)\n","test_neg_sentences, test_neg_sentiments = load_data_from_dir(test_neg_dir, 0)\n","\n","# Combine data\n","train_sentences = train_pos_sentences + train_neg_sentences\n","train_sentiments = train_pos_sentiments + train_neg_sentiments\n","test_sentences = test_pos_sentences + test_neg_sentences\n","test_sentiments = test_pos_sentiments + test_neg_sentiments\n","\n","# Tokenization and padding\n","tokenizer = Tokenizer(num_words=5000)\n","tokenizer.fit_on_texts(train_sentences + test_sentences)\n","train_sequences = tokenizer.texts_to_sequences(train_sentences)\n","test_sequences = tokenizer.texts_to_sequences(test_sentences)\n","word_index = tokenizer.word_index\n","\n","MAX_SEQUENCE_LENGTH = 100\n","x_train = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","x_test = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# Convert sentiments to numpy arrays\n","y_train = np.array(train_sentiments)\n","y_test = np.array(test_sentiments)\n","\n","# Convert to categorical\n","y_train = to_categorical(y_train, num_classes=2)\n","y_test = to_categorical(y_test, num_classes=2)\n"],"metadata":{"id":"zveLh7IxNGt7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download GloVe embeddings\n","!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip glove.6B.zip\n","\n","# Load GloVe embeddings\n","embeddings_index = {}\n","with open('glove.6B.100d.txt', encoding='utf-8') as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs"],"metadata":{"id":"uUTpmcIdj59h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create embedding matrix\n","VOCAB_SIZE = len(tokenizer.word_index) + 1\n","EMBEDDING_DIM = 100\n","embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n","for word, i in tokenizer.word_index.items():\n","    if i < VOCAB_SIZE:\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","\n","# Embedding layer with pre-trained GloVe embeddings\n","embedding_layer = Embedding(input_dim=VOCAB_SIZE,\n","                            output_dim=EMBEDDING_DIM,\n","                            weights=[embedding_matrix],\n","                            input_length=MAX_SEQUENCE_LENGTH,\n","                            trainable=False)\n","\n","# Data augmentation function\n","def augment_text(text):\n","    # Simple augmentation by synonym replacement or other techniques\n","    words = text_to_word_sequence(text)\n","    augmented_text = ' '.join(words)  # For simplicity, we are not changing the text\n","    return augmented_text\n","\n","# Apply data augmentation\n","augmented_train_sentences = [augment_text(sentence) for sentence in train_sentences]\n","augmented_train_sequences = tokenizer.texts_to_sequences(augmented_train_sentences)\n","x_train_augmented = pad_sequences(augmented_train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# Combine original and augmented data\n","x_train_combined = np.vstack((x_train, x_train_augmented))\n","y_train_combined = np.vstack((y_train, y_train))"],"metadata":{"id":"r2w2yinokKIT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aspect data handling\n","lab_dir = os.path.join(data_dir, 'lab')\n","sentences = []\n","aspects = []\n","sentiments = []\n","\n","for file_name in os.listdir(lab_dir):\n","    if file_name.endswith(\".txt\"):\n","        with open(os.path.join(lab_dir, file_name), 'r', encoding='utf-8') as file:\n","            for line in file:\n","                parts = line.strip().split('\",')\n","                if len(parts) < 2:\n","                    continue\n","                sentence = parts[0][1:]\n","                aspect_sentiment = parts[1].split(',')\n","                if len(aspect_sentiment) < 2:\n","                    continue\n","                aspect = aspect_sentiment[0]\n","                sentiment = aspect_sentiment[1]\n","                sentences.append(sentence)\n","                aspects.append(aspect)\n","                sentiments.append(sentiment)\n","\n","df = pd.DataFrame({\n","    'Sentence': sentences,\n","    'Aspect': aspects,\n","    'Sentiment': sentiments\n","})\n","\n","# Label encoding\n","le = LabelEncoder()\n","df['Sentiment'] = le.fit_transform(df['Sentiment'])\n","\n","# Tokenize sentences\n","tokenizer = Tokenizer(num_words=20000)\n","tokenizer.fit_on_texts(df['Sentence'])\n","labeled_sequences = tokenizer.texts_to_sequences(df['Sentence'])\n","MAX_SEQUENCE_LENGTH = 128\n","labeled_data = pad_sequences(labeled_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# Aspect embedding\n","aspect_dict = {\n","    'Writing': [1, 0, 0, 0, 0, 0, 0, 0],\n","    'Acting': [0, 1, 0, 0, 0, 0, 0, 0],\n","    'Directing': [0, 0, 1, 0, 0, 0, 0, 0],\n","    'Cinematography': [0, 0, 0, 1, 0, 0, 0, 0],\n","    'Production': [0, 0, 0, 0, 1, 0, 0, 0],\n","    'Overall': [0, 0, 0, 0, 0, 1, 0, 0],\n","    'Music': [0, 0, 0, 0, 0, 0, 1, 0],\n","    'Actors': [0, 0, 0, 0, 0, 0, 0, 1],\n","    'Direction': [0, 0, 0, 0, 0, 0, 0, 0]\n","}\n","\n","def get_aspect_embedding(aspect):\n","    if aspect in aspect_dict:\n","        return aspect_dict[aspect]\n","    else:\n","        return [0] * len(aspect_dict['Writing'])\n","\n","df['Aspect_Embedding'] = df['Aspect'].apply(lambda x: get_aspect_embedding(x))\n","\n","aspect_data = np.array(df['Aspect_Embedding'].tolist())\n","aspect_labels = to_categorical(df['Sentiment'], num_classes=3)\n","\n","# Split data\n","labeled_x_train, labeled_x_test, labeled_y_train, labeled_y_test, labeled_aspect_train, labeled_aspect_test = train_test_split(\n","    labeled_data, aspect_labels, aspect_data, test_size=0.2, random_state=42)"],"metadata":{"id":"K8rog4TQXvdM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model architecture\n","review_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n","aspect_input = Input(shape=(ASPECT_EMBEDDING_DIM,), dtype='float32')\n","\n","# Embedding layer\n","embedded_sequences = embedding_layer(review_input)\n","\n","# Convolutional layer\n","conv_layer = Conv1D(filters=128, kernel_size=5, activation='relu')(embedded_sequences)\n","conv_layer = Dropout(0.5)(conv_layer)\n","pooled_output = GlobalMaxPooling1D()(conv_layer)\n","\n","# Aspect attention layer\n","attention = Dense(EMBEDDING_DIM, activation='tanh')(aspect_input)\n","attention = Dense(1, activation='softmax')(attention)\n","attention_output = attention * pooled_output\n","\n","# Concatenate the outputs\n","merged_output = Concatenate()([attention_output, aspect_input])\n","\n","# Fully connected layers\n","dense_output = Dense(64, activation='relu')(merged_output)\n","dense_output = Dropout(0.5)(dense_output)\n","predictions = Dense(3, activation='softmax')(dense_output)\n","\n"],"metadata":{"id":"sXvOt-AHYrUd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build the model\n","model = Model(inputs=[review_input, aspect_input], outputs=predictions)\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Model summary\n","model.summary()"],"metadata":{"id":"YSK6wIXZkbvS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","history = model.fit([x_train_combined, labeled_aspect_train], y_train_combined,\n","                    validation_data=([x_test, labeled_aspect_test], y_test),\n","                    epochs=10, batch_size=32)"],"metadata":{"id":"qVZY4edRNLHr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the model\n","loss, accuracy = model.evaluate([labeled_x_test, labeled_aspect_test], labeled_y_test)\n","print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"],"metadata":{"id":"Ymxtrak9NNz7"},"execution_count":null,"outputs":[]}]}