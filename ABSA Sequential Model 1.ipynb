{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM+ZjcmWVWwhTtznd1J+3V+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"KLvTF9tNqYqA"},"outputs":[],"source":["import os\n","import numpy as np\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Flatten\n","from keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"xOfIsu2HqiRe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to load dataset from a directory\n","def load_data_from_dir(data_dir, sentiment):\n","    sentences = []\n","    sentiments = []\n","    for file_name in os.listdir(data_dir):\n","        if file_name.endswith(\".txt\"):\n","            with open(os.path.join(data_dir, file_name), 'r', encoding='utf-8') as file:\n","                sentences.append(file.read())\n","                sentiments.append(sentiment)\n","    return sentences, sentiments\n","\n","# Data directories\n","data_dir = '/content/drive/My Drive/ML Assinment 2/assinment 2 part 2/Project/aclImdb/'\n","train_pos_dir = os.path.join(data_dir, 'train/pos')\n","train_neg_dir = os.path.join(data_dir, 'train/neg')\n","test_pos_dir = os.path.join(data_dir, 'test/pos')\n","test_neg_dir = os.path.join(data_dir, 'test/neg')\n","\n","# Load data\n","train_pos_sentences, train_pos_sentiments = load_data_from_dir(train_pos_dir, 1)\n","train_neg_sentences, train_neg_sentiments = load_data_from_dir(train_neg_dir, 0)\n","test_pos_sentences, test_pos_sentiments = load_data_from_dir(test_pos_dir, 1)\n","test_neg_sentences, test_neg_sentiments = load_data_from_dir(test_neg_dir, 0)\n","\n","# Combine data\n","train_sentences = train_pos_sentences + train_neg_sentences\n","train_sentiments = train_pos_sentiments + train_neg_sentiments\n","test_sentences = test_pos_sentences + test_neg_sentiments\n","test_sentiments = test_pos_sentiments + test_neg_sentiments\n","\n","# Convert sentiments to numpy arrays\n","y_train = np.array(train_sentiments)\n","y_test = np.array(test_sentiments)\n","\n","# Convert to categorical\n","y_train = to_categorical(y_train, num_classes=2)\n","y_test = to_categorical(y_test, num_classes=2)\n","\n","# Tokenization and padding\n","tokenizer = Tokenizer(num_words=5000)\n","tokenizer.fit_on_texts(train_sentences + test_sentences)\n","train_sequences = tokenizer.texts_to_sequences(train_sentences)\n","test_sequences = tokenizer.texts_to_sequences(test_sentences)\n","word_index = tokenizer.word_index\n","\n","MAX_SEQUENCE_LENGTH = 100\n","x_train = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","x_test = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n"],"metadata":{"id":"kcgGKGKWqnye"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to load dataset from a directory\n","def load_data_from_dir(data_dir, sentiment):\n","    sentences = []\n","    sentiments = []\n","    for file_name in os.listdir(data_dir):\n","        if file_name.endswith(\".txt\"):\n","            with open(os.path.join(data_dir, file_name), 'r', encoding='utf-8') as file:\n","                sentences.append(file.read())\n","                sentiments.append(sentiment)\n","    return sentences, sentiments\n","\n","# Data directories\n","data_dir = '/content/drive/My Drive/ML Assinment 2/assinment 2 part 2/Project/aclImdb/'\n","train_pos_dir = os.path.join(data_dir, 'train/pos')\n","train_neg_dir = os.path.join(data_dir, 'train/neg')\n","test_pos_dir = os.path.join(data_dir, 'test/pos')\n","test_neg_dir = os.path.join(data_dir, 'test/neg')\n","\n","# Load data\n","train_pos_sentences, train_pos_sentiments = load_data_from_dir(train_pos_dir, 1)\n","train_neg_sentences, train_neg_sentiments = load_data_from_dir(train_neg_dir, 0)\n","test_pos_sentences, test_pos_sentiments = load_data_from_dir(test_pos_dir, 1)\n","test_neg_sentences, test_neg_sentiments = load_data_from_dir(test_neg_dir, 0)\n","\n","# Combine data\n","train_sentences = train_pos_sentences + train_neg_sentences\n","train_sentiments = train_pos_sentiments + train_neg_sentiments\n","test_sentences = test_pos_sentences + test_neg_sentiments\n","test_sentiments = test_pos_sentiments + test_neg_sentiments\n","\n","# Convert sentiments to numpy arrays\n","y_train = np.array(train_sentiments)\n","y_test = np.array(test_sentiments)\n","\n","# Convert to categorical\n","y_train = to_categorical(y_train, num_classes=2)\n","y_test = to_categorical(y_test, num_classes=2)\n","\n","# Tokenization and padding\n","tokenizer = Tokenizer(num_words=5000)\n","tokenizer.fit_on_texts(train_sentences + test_sentences)\n","train_sequences = tokenizer.texts_to_sequences(train_sentences)\n","test_sequences = tokenizer.texts_to_sequences(test_sentences)\n","word_index = tokenizer.word_index\n","\n","MAX_SEQUENCE_LENGTH = 100\n","x_train = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","x_test = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n"],"metadata":{"id":"LZDwZuenquE-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download GloVe embeddings\n","!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip glove.6B.zip\n","\n","# Load GloVe embeddings\n","embeddings_index = {}\n","with open('glove.6B.100d.txt', encoding='utf-8') as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs\n","\n","# Create embedding matrix\n","VOCAB_SIZE = len(tokenizer.word_index) + 1\n","EMBEDDING_DIM = 100\n","embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n","for word, i in tokenizer.word_index.items():\n","    if i < VOCAB_SIZE:\n","        embedding_vector = embeddings_index.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","\n","# Embedding layer with pre-trained GloVe embeddings\n","embedding_layer = Embedding(input_dim=VOCAB_SIZE,\n","                            output_dim=EMBEDDING_DIM,\n","                            weights=[embedding_matrix],\n","                            input_length=MAX_SEQUENCE_LENGTH,\n","                            trainable=False)\n","\n"],"metadata":{"id":"1_MglMn7qwXW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Aspect embedding dictionary\n","aspect_dict = {\n","    'Writing': [1, 0, 0, 0, 0, 0, 0, 0],\n","    'Acting': [0, 1, 0, 0, 0, 0, 0, 0],\n","    'Directing': [0, 0, 1, 0, 0, 0, 0, 0],\n","    'Cinematography': [0, 0, 0, 1, 0, 0, 0, 0],\n","    'Production': [0, 0, 0, 0, 1, 0, 0, 0],\n","    'Overall': [0, 0, 0, 0, 0, 1, 0, 0],\n","    'Music': [0, 0, 0, 0, 0, 0, 1, 0],\n","    'Actors': [0, 0, 0, 0, 0, 0, 0, 1]\n","}\n","\n","def get_aspect_embedding(aspect):\n","    if aspect in aspect_dict:\n","        return aspect_dict[aspect]\n","    else:\n","        return [0] * len(aspect_dict['Writing'])\n","\n","# Dummy aspects for illustration (you would typically have these in your dataset)\n","aspects = ['Writing'] * len(train_sentences)  # Example with all 'Writing' aspects\n","\n","# Get aspect embeddings\n","aspect_embeddings = np.array([get_aspect_embedding(aspect) for aspect in aspects])\n","\n","# Define the Sequential model\n","input_review = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='input_review')\n","input_aspect = Input(shape=(len(aspect_dict['Writing']),), dtype='float32', name='input_aspect')\n","\n","embedded_sequences = embedding_layer(input_review)\n","\n","conv_layer = Conv1D(filters=128, kernel_size=5, activation='relu')(embedded_sequences)\n","pooled_output = GlobalMaxPooling1D()(conv_layer)\n","\n","merged_output = Concatenate()([pooled_output, input_aspect])\n","\n","dense = Dense(64, activation='relu')(merged_output)\n","dropout = Dropout(0.5)(dense)\n","output = Dense(2, activation='softmax')(dropout)\n"],"metadata":{"id":"gqqdRgs9q1uO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Model(inputs=[input_review, input_aspect], outputs=output)\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Model summary\n","model.summary()\n","\n"],"metadata":{"id":"bYqTpQN2rive"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","history = model.fit([x_train, aspect_embeddings], y_train, validation_data=([x_test, aspect_embeddings[:len(x_test)]], y_test), epochs=10, batch_size=32)"],"metadata":{"id":"T-SJ79vkrm2c"},"execution_count":null,"outputs":[]}]}