{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNb2QsU3bTT4xXB3pltIDsb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KHEiQYmhKkD7","executionInfo":{"status":"ok","timestamp":1716967047310,"user_tz":-600,"elapsed":32560,"user":{"displayName":"Arsh.T","userId":"00431916674490476738"}},"outputId":"b1f42398-ddff-447e-aeba-1a022ae87964"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Attention, Concatenate\n","from keras.models import Model\n","from keras.utils import to_categorical\n"],"metadata":{"id":"Rx-djo5cKwQP","executionInfo":{"status":"ok","timestamp":1716967106605,"user_tz":-600,"elapsed":6088,"user":{"displayName":"Arsh.T","userId":"00431916674490476738"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["data_dir = '/content/drive/My Drive/ML Assinment 2/assinment 2 part 2/Project/aclImdb/'\n","\n","# Function to load dataset from a directory\n","def load_data_from_dir(data_dir, sentiment):\n","    sentences = []\n","    sentiments = []\n","    for file_name in os.listdir(data_dir):\n","        if file_name.endswith(\".txt\"):\n","            with open(os.path.join(data_dir, file_name), 'r', encoding='utf-8') as file:\n","                sentences.append(file.read())\n","                sentiments.append(sentiment)\n","    return sentences, sentiments\n","\n","# Load train and test data\n","train_pos_dir = os.path.join(data_dir, 'train/pos')\n","train_neg_dir = os.path.join(data_dir, 'train/neg')\n","test_pos_dir = os.path.join(data_dir, 'test/pos')\n","test_neg_dir = os.path.join(data_dir, 'test/neg')\n","\n","train_pos_sentences, train_pos_sentiments = load_data_from_dir(train_pos_dir, 1)\n","train_neg_sentences, train_neg_sentiments = load_data_from_dir(train_neg_dir, 0)\n","test_pos_sentences, test_pos_sentiments = load_data_from_dir(test_pos_dir, 1)\n","test_neg_sentences, test_neg_sentiments = load_data_from_dir(test_neg_dir, 0)\n","\n","# Combine the data\n","train_sentences = train_pos_sentences + train_neg_sentences\n","train_sentiments = train_pos_sentiments + train_neg_sentiments\n","test_sentences = test_pos_sentences + test_neg_sentences\n","test_sentiments = test_pos_sentiments + test_neg_sentiments\n","\n","# Tokenize sentences\n","tokenizer = Tokenizer(num_words=5000)\n","tokenizer.fit_on_texts(train_sentences + test_sentences)\n","train_sequences = tokenizer.texts_to_sequences(train_sentences)\n","test_sequences = tokenizer.texts_to_sequences(test_sentences)\n","word_index = tokenizer.word_index\n","\n","MAX_SEQUENCE_LENGTH = 100\n","x_train = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","x_test = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# Convert sentiments to numpy arrays\n","y_train = np.array(train_sentiments)\n","y_test = np.array(test_sentiments)\n","\n","# Convert to categorical\n","y_train = to_categorical(y_train, num_classes=2)\n","y_test = to_categorical(y_test, num_classes=2)\n"],"metadata":{"id":"zveLh7IxNGt7","executionInfo":{"status":"ok","timestamp":1716969005673,"user_tz":-600,"elapsed":1501897,"user":{"displayName":"Arsh.T","userId":"00431916674490476738"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["lab_dir = os.path.join(data_dir, 'lab')\n","sentences = []\n","aspects = []\n","sentiments = []\n","\n","# Read labeled data\n","for file_name in os.listdir(lab_dir):\n","    if file_name.endswith(\".txt\"):\n","        with open(os.path.join(lab_dir, file_name), 'r', encoding='utf-8') as file:\n","            for line in file:\n","                parts = line.strip().split('\",')\n","                if len(parts) < 2:\n","                    continue  # Skip lines that do not have the expected format\n","                sentence = parts[0][1:]\n","                aspect_sentiment = parts[1].split(',')\n","                if len(aspect_sentiment) < 2:\n","                    continue  # Skip parts that do not have the expected format\n","                aspect = aspect_sentiment[0]\n","                sentiment = aspect_sentiment[1]\n","\n","                sentences.append(sentence)\n","                aspects.append(aspect)\n","                sentiments.append(sentiment)\n","\n","# Convert to DataFrame for easier manipulation\n","df = pd.DataFrame({\n","    'Sentence': sentences,\n","    'Aspect': aspects,\n","    'Sentiment': sentiments\n","})\n","\n","# Preprocess the labels\n","le = LabelEncoder()\n","df['Sentiment'] = le.fit_transform(df['Sentiment'])\n","\n","# Tokenize sentences using Keras Tokenizer\n","tokenizer = Tokenizer(num_words=20000)\n","tokenizer.fit_on_texts(df['Sentence'])\n","labeled_sequences = tokenizer.texts_to_sequences(df['Sentence'])\n","MAX_SEQUENCE_LENGTH = 128\n","labeled_data = pad_sequences(labeled_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# Aspect embedding\n","aspect_dict = {\n","    'Writing': [1, 0, 0, 0, 0, 0, 0, 0],\n","    'Acting': [0, 1, 0, 0, 0, 0, 0, 0],\n","    'Directing': [0, 0, 1, 0, 0, 0, 0, 0],\n","    'Cinematography': [0, 0, 0, 1, 0, 0, 0, 0],\n","    'Production': [0, 0, 0, 0, 1, 0, 0, 0],\n","    'Overall': [0, 0, 0, 0, 0, 1, 0, 0],\n","    'Music': [0, 0, 0, 0, 0, 0, 1, 0],\n","    'Actors': [0, 0, 0, 0, 0, 0, 0, 1],\n","    'Direction': [0, 0, 0, 0, 0, 0, 0, 0]\n","}\n","\n","# Function to handle unknown aspects\n","def get_aspect_embedding(aspect):\n","    if aspect in aspect_dict:\n","        return aspect_dict[aspect]\n","    else:\n","        print(f\"Unknown aspect: {aspect}\")\n","        return [0] * len(aspect_dict['Writing'])  # Default embedding for unknown aspects\n","\n","# Apply the function to get aspect embeddings\n","df['Aspect_Embedding'] = df['Aspect'].apply(lambda x: get_aspect_embedding(x))\n","\n","# Prepare aspect embeddings\n","aspect_data = np.array(df['Aspect_Embedding'].tolist())\n","\n","# Prepare labels\n","aspect_labels = to_categorical(df['Sentiment'], num_classes=3)\n","\n","# Split the data\n","labeled_x_train, labeled_x_test, labeled_y_train, labeled_y_test, labeled_aspect_train, labeled_aspect_test = train_test_split(\n","    labeled_data, aspect_labels, aspect_data, test_size=0.2, random_state=42)"],"metadata":{"id":"K8rog4TQXvdM","executionInfo":{"status":"ok","timestamp":1716969865447,"user_tz":-600,"elapsed":5,"user":{"displayName":"Arsh.T","userId":"00431916674490476738"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Assuming 'word_index' is defined from the tokenizer and 'MAX_SEQUENCE_LENGTH' is already set\n","\n","VOCAB_SIZE = len(tokenizer.word_index) + 1  # Ensure word_index is from the tokenizer used previously\n","EMBEDDING_DIM = 100\n","ASPECT_EMBEDDING_DIM = 8  # Updated to match the length of aspect embeddings defined previously\n","NUM_CLASSES = 3\n","\n","# Input layers\n","review_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n","aspect_input = Input(shape=(ASPECT_EMBEDDING_DIM,), dtype='float32')\n","\n","# Embedding layer\n","embedded_sequences = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(review_input)\n","\n","# Convolutional layer\n","conv_layer = Conv1D(filters=128, kernel_size=5, activation='relu')(embedded_sequences)\n","pooled_output = GlobalMaxPooling1D()(conv_layer)\n","\n","# Aspect attention layer\n","attention = Dense(EMBEDDING_DIM, activation='tanh')(aspect_input)\n","attention = Dense(1, activation='softmax')(attention)\n","attention_output = attention * pooled_output\n","\n","# Concatenate the outputs\n","merged_output = Concatenate()([attention_output, aspect_input])\n","\n","# Fully connected layers\n","dense_output = Dense(64, activation='relu')(merged_output)\n","predictions = Dense(NUM_CLASSES, activation='softmax')(dense_output)\n","\n","# Build the model\n","model = Model(inputs=[review_input, aspect_input], outputs=predictions)\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Model summary\n","model.summary()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sXvOt-AHYrUd","executionInfo":{"status":"ok","timestamp":1716969982848,"user_tz":-600,"elapsed":1111,"user":{"displayName":"Arsh.T","userId":"00431916674490476738"}},"outputId":"701bce64-47f7-4283-be09-3655694a3efe"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_1 (InputLayer)        [(None, 128)]                0         []                            \n","                                                                                                  \n"," input_2 (InputLayer)        [(None, 8)]                  0         []                            \n","                                                                                                  \n"," embedding (Embedding)       (None, 128, 100)             33200     ['input_1[0][0]']             \n","                                                                                                  \n"," dense (Dense)               (None, 100)                  900       ['input_2[0][0]']             \n","                                                                                                  \n"," conv1d (Conv1D)             (None, 124, 128)             64128     ['embedding[0][0]']           \n","                                                                                                  \n"," dense_1 (Dense)             (None, 1)                    101       ['dense[0][0]']               \n","                                                                                                  \n"," global_max_pooling1d (Glob  (None, 128)                  0         ['conv1d[0][0]']              \n"," alMaxPooling1D)                                                                                  \n","                                                                                                  \n"," tf.math.multiply (TFOpLamb  (None, 128)                  0         ['dense_1[0][0]',             \n"," da)                                                                 'global_max_pooling1d[0][0]']\n","                                                                                                  \n"," concatenate (Concatenate)   (None, 136)                  0         ['tf.math.multiply[0][0]',    \n","                                                                     'input_2[0][0]']             \n","                                                                                                  \n"," dense_2 (Dense)             (None, 64)                   8768      ['concatenate[0][0]']         \n","                                                                                                  \n"," dense_3 (Dense)             (None, 3)                    195       ['dense_2[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 107292 (419.11 KB)\n","Trainable params: 107292 (419.11 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Train the model\n","history = model.fit([labeled_x_train, labeled_aspect_train], labeled_y_train, epochs=10, batch_size=32, validation_split=0.2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qVZY4edRNLHr","executionInfo":{"status":"ok","timestamp":1716970018379,"user_tz":-600,"elapsed":3001,"user":{"displayName":"Arsh.T","userId":"00431916674490476738"}},"outputId":"5297c694-e08b-41a0-f773-9c4a0ac16d50"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","1/1 [==============================] - 2s 2s/step - loss: 1.0931 - accuracy: 0.4500 - val_loss: 1.0813 - val_accuracy: 0.3333\n","Epoch 2/10\n","1/1 [==============================] - 0s 64ms/step - loss: 1.0393 - accuracy: 0.7000 - val_loss: 1.0600 - val_accuracy: 0.3333\n","Epoch 3/10\n","1/1 [==============================] - 0s 90ms/step - loss: 0.9924 - accuracy: 0.7500 - val_loss: 1.0411 - val_accuracy: 0.3333\n","Epoch 4/10\n","1/1 [==============================] - 0s 63ms/step - loss: 0.9503 - accuracy: 0.7500 - val_loss: 1.0230 - val_accuracy: 0.5000\n","Epoch 5/10\n","1/1 [==============================] - 0s 65ms/step - loss: 0.9121 - accuracy: 0.7500 - val_loss: 1.0065 - val_accuracy: 0.5000\n","Epoch 6/10\n","1/1 [==============================] - 0s 62ms/step - loss: 0.8759 - accuracy: 0.8000 - val_loss: 0.9906 - val_accuracy: 0.5000\n","Epoch 7/10\n","1/1 [==============================] - 0s 75ms/step - loss: 0.8394 - accuracy: 0.8000 - val_loss: 0.9752 - val_accuracy: 0.5000\n","Epoch 8/10\n","1/1 [==============================] - 0s 79ms/step - loss: 0.8033 - accuracy: 0.8500 - val_loss: 0.9610 - val_accuracy: 0.5000\n","Epoch 9/10\n","1/1 [==============================] - 0s 60ms/step - loss: 0.7679 - accuracy: 0.8500 - val_loss: 0.9481 - val_accuracy: 0.5000\n","Epoch 10/10\n","1/1 [==============================] - 0s 72ms/step - loss: 0.7327 - accuracy: 0.8500 - val_loss: 0.9365 - val_accuracy: 0.5000\n"]}]},{"cell_type":"code","source":["# Evaluate the model\n","loss, accuracy = model.evaluate([labeled_x_test, labeled_aspect_test], labeled_y_test)\n","print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ymxtrak9NNz7","executionInfo":{"status":"ok","timestamp":1716972287058,"user_tz":-600,"elapsed":398,"user":{"displayName":"Arsh.T","userId":"00431916674490476738"}},"outputId":"33e810a0-664f-490b-b086-e61b0c9e79f8"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 35ms/step - loss: 1.0590 - accuracy: 0.4286\n","Test Accuracy: 42.86%\n"]}]}]}