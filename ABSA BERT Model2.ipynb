{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNKbruhxGz/1NxMcQ+9kCEy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lPL-EEQCRkV8","executionInfo":{"status":"ok","timestamp":1716972277053,"user_tz":-600,"elapsed":92008,"user":{"displayName":"Arsh.T","userId":"00431916674490476738"}},"outputId":"203961a2-d3df-47b4-8792-153794721fef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, TFBertForSequenceClassification\n","from transformers import InputExample, InputFeatures\n","import tensorflow as tf\n"],"metadata":{"id":"t9WFtYNZRvW2","executionInfo":{"status":"ok","timestamp":1716972308564,"user_tz":-600,"elapsed":11239,"user":{"displayName":"Arsh.T","userId":"00431916674490476738"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["data_dir = '/content/drive/My Drive/ML Assinment 2/assinment 2 part 2/Project/aclImdb/'\n","\n","# Function to load dataset from a directory\n","def load_data_from_dir(data_dir, sentiment):\n","    sentences = []\n","    sentiments = []\n","    for file_name in os.listdir(data_dir):\n","        if file_name.endswith(\".txt\"):\n","            with open(os.path.join(data_dir, file_name), 'r', encoding='utf-8') as file:\n","                sentences.append(file.read())\n","                sentiments.append(sentiment)\n","    return sentences, sentiments\n","\n","# Load train and test data\n","train_pos_dir = os.path.join(data_dir, 'train/pos')\n","train_neg_dir = os.path.join(data_dir, 'train/neg')\n","test_pos_dir = os.path.join(data_dir, 'test/pos')\n","test_neg_dir = os.path.join(data_dir, 'test/neg')\n","\n","train_pos_sentences, train_pos_sentiments = load_data_from_dir(train_pos_dir, 1)\n","train_neg_sentences, train_neg_sentiments = load_data_from_dir(train_neg_dir, 0)\n","test_pos_sentences, test_pos_sentiments = load_data_from_dir(test_pos_dir, 1)\n","test_neg_sentences, test_neg_sentiments = load_data_from_dir(test_neg_dir, 0)\n","\n","# Combine the data\n","train_sentences = train_pos_sentences + train_neg_sentences\n","train_sentiments = train_pos_sentiments + train_neg_sentiments\n","test_sentences = test_pos_sentences + test_neg_sentiments\n","test_sentiments = test_pos_sentiments + test_neg_sentiments\n","\n","# Convert to DataFrame for easier manipulation\n","df_train = pd.DataFrame({'sentence': train_sentences, 'label': train_sentiments})\n","df_test = pd.DataFrame({'sentence': test_sentences, 'label': test_sentiments})\n","\n","\n","# Function to load dataset from a directory\n","def load_data_from_dir(data_dir, sentiment):\n","    sentences = []\n","    sentiments = []\n","    for file_name in os.listdir(data_dir):\n","        if file_name.endswith(\".txt\"):\n","            with open(os.path.join(data_dir, file_name), 'r', encoding='utf-8') as file:\n","                sentences.append(file.read())\n","                sentiments.append(sentiment)\n","    return sentences, sentiments\n","\n","# Load train and test data\n","train_pos_dir = os.path.join(data_dir, 'train/pos')\n","train_neg_dir = os.path.join(data_dir, 'train/neg')\n","test_pos_dir = os.path.join(data_dir, 'test/pos')\n","test_neg_dir = os.path.join(data_dir, 'test/neg')\n","\n","train_pos_sentences, train_pos_sentiments = load_data_from_dir(train_pos_dir, 1)\n","train_neg_sentences, train_neg_sentiments = load_data_from_dir(train_neg_dir, 0)\n","test_pos_sentences, test_pos_sentiments = load_data_from_dir(test_pos_dir, 1)\n","test_neg_sentences, test_neg_sentiments = load_data_from_dir(test_neg_dir, 0)\n","\n","# Combine the data\n","train_sentences = train_pos_sentences + train_neg_sentences\n","train_sentiments = train_pos_sentiments + train_neg_sentiments\n","test_sentences = test_pos_sentences + test_neg_sentiments\n","test_sentiments = test_pos_sentiments + test_neg_sentiments\n","\n","# Convert to DataFrame for easier manipulation\n","df_train = pd.DataFrame({'sentence': train_sentences, 'label': train_sentiments})\n","df_test = pd.DataFrame({'sentence': test_sentences, 'label': test_sentiments})\n"],"metadata":{"id":"5vhiMoKbRwjO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labeled_dir = os.path.join(data_dir, 'labeled')\n","sentences = []\n","aspects = []\n","sentiments = []\n","\n","# Read labeled data\n","for file_name in os.listdir(labeled_dir):\n","    if file_name.endswith(\".txt\"):\n","        with open(os.path.join(labeled_dir, file_name), 'r', encoding='utf-8') as file:\n","            for line in file:\n","                parts = line.strip().split('\",')\n","                sentence = parts[0][1:]\n","                aspect_sentiment = parts[1].split(',')\n","                aspect = aspect_sentiment[0]\n","                sentiment = aspect_sentiment[1]\n","\n","                sentences.append(sentence)\n","                aspects.append(aspect)\n","                sentiments.append(sentiment)\n","\n","# Convert to DataFrame for easier manipulation\n","df = pd.DataFrame({\n","    'Sentence': sentences,\n","    'Aspect': aspects,\n","    'Sentiment': sentiments\n","})\n","\n","# Preprocess the labels\n","le = LabelEncoder()\n","df['Sentiment'] = le.fit_transform(df['Sentiment'])\n","\n","# Create aspect embedding\n","aspect_dict = {\n","    'Writing': 0,\n","    'Acting': 1,\n","    'Directing': 2,\n","    'Cinematography': 3,\n","    'Production': 4\n","}\n","df['Aspect'] = df['Aspect'].apply(lambda x: aspect_dict[x])\n","df['Aspect'] = df['Aspect'].astype(str)\n","\n","# Combine sentence and aspect for training\n","df['Combined'] = df['Aspect'] + \" [SEP] \" + df['Sentence']\n","\n","# Split the data\n","train, test = train_test_split(df, test_size=0.2, random_state=42)\n","train_examples = train.apply(lambda x: InputExample(guid=None, text_a=x['Combined'], text_b=None, label=x['Sentiment']), axis=1)\n","test_examples = test.apply(lambda x: InputExample(guid=None, text_a=x['Combined'], text_b=None, label=x['Sentiment']), axis=1)\n"],"metadata":{"id":"u8hCKtBCR3Bw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to convert DataFrame to InputFeatures\n","def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n","    features = []\n","\n","    for e in examples:\n","        input_dict = tokenizer.encode_plus(\n","            e.text_a,\n","            add_special_tokens=True,\n","            max_length=max_length,\n","            pad_to_max_length=True,\n","            return_token_type_ids=True,\n","            return_attention_mask=True,\n","            truncation=True\n","        )\n","\n","        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"], input_dict[\"token_type_ids\"], input_dict[\"attention_mask\"])\n","\n","        features.append(\n","            InputFeatures(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                token_type_ids=token_type_ids,\n","                label=e.label\n","            )\n","        )\n","\n","    def gen():\n","        for f in features:\n","            yield (\n","                {\n","                    \"input_ids\": f.input_ids,\n","                    \"attention_mask\": f.attention_mask,\n","                    \"token_type_ids\": f.token_type_ids,\n","                },\n","                f.label,\n","            )\n","\n","    return tf.data.Dataset.from_generator(\n","        gen,\n","        (\n","            {\n","                \"input_ids\": tf.int32,\n","                \"attention_mask\": tf.int32,\n","                \"token_type_ids\": tf.int32,\n","            },\n","            tf.int64,\n","        ),\n","        (\n","            {\n","                \"input_ids\": tf.TensorShape([None]),\n","                \"attention_mask\": tf.TensorShape([None]),\n","                \"token_type_ids\": tf.TensorShape([None]),\n","            },\n","            tf.TensorShape([]),\n","        ),\n","    )\n"],"metadata":{"id":"e51W9r16R4xF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Convert examples to TensorFlow dataset\n","train_dataset = convert_examples_to_tf_dataset(list(train_examples), tokenizer)\n","test_dataset = convert_examples_to_tf_dataset(list(test_examples), tokenizer)\n","\n","train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)\n","test_dataset = test_dataset.batch(32)\n"],"metadata":{"id":"sXUWN7NGR7XV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the BERT model\n","model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n","\n","# Compile the model\n","optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","\n","model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n","\n","# Model summary\n","model.summary()\n"],"metadata":{"id":"JOT-ATZWR9Nt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","history = model.fit(train_dataset, epochs=2, validation_data=test_dataset)\n"],"metadata":{"id":"FAKVafKmR_VN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the model\n","loss, accuracy = model.evaluate(test_dataset)\n","print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"],"metadata":{"id":"7yl0n9MgSA4e"},"execution_count":null,"outputs":[]}]}